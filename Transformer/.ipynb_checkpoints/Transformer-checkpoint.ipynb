{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81349437-8743-4db9-80b7-04ba0795c19e",
   "metadata": {},
   "source": [
    "# Transformer代码详解\n",
    "本文主要根据[知乎：深度学习-图解Transformer](https://zhuanlan.zhihu.com/p/105493618)系列得到\n",
    "\n",
    "## 零、 问题总结\n",
    "> **Question 1:** Attention机制为什么要设计QK即可计算得分，为何还要最后乘以V？<br/>\n",
    "> **Answer:** 我们可以认为对于目标$z_i$的得分可以表示为源词汇表$\\text{src\\_vocab} = {x_1, \\ldots, x_s}$的一系列权重, 即\n",
    "> $$z_i = \\sum_{j \\in src\\_vocab} score_j x_j$$\n",
    "> 根据上式，V表示$x_j$，Q和K乘积得到score，并且为了消除维度的影响，score除以$\\sqrt{d_{model}}$。这样做可以保持V的相对固定，因为是对V的成倍放缩。具体而言，分为以下两种情况:\n",
    "> + 当计算Self-Attention时，Q,K,V均是同一来源(src或tgt)\n",
    "> + 当计算Encoder-Decoder Attention时，K,V来源于src， Q来源于tgt\n",
    "\n",
    "> **Question 2:** Transformer中Attention共有几类？以及它们对应的Q,K,V,mask都是什么？ <br/>\n",
    "> **Answer:** 共有三类\n",
    "> + Encoder里的Self-Attention：计算src序列的自注意力它的输入 query, key, value 均来自 src，维度为 (batch_size * src_seq_len * d_model)， mask 维度为 (batch_size * 1 * src_seq_len)。mask的目的是屏蔽掉填充位\n",
    ">     + attention的输入为 query, key, value 的维度为(batch_size * head_num * src_seq_len * head_dim)，mask维度为(batch_size * 1 * 1 * src_seq_len)\n",
    ">     + attention中score的维度为 batch_size * head_num * src_seq_len* src_seq_len\n",
    "> + Decoder里的Self-Attention：计算tgt序列的自注意力，它的输入 query, key, value 均来自 tgt，维度为(batch_size * tgt_seq_len * d_model)， mask的维度为 (batch_size * tgt_seq_len * tgt_seq_len)。mask这里是下三角矩阵，旨在屏蔽当前时刻之后的序列\n",
    ">     + attention的输入为 query, key, value 的维度为(batch_size * head_num * tgt_seq_len * head_dim)，mask维度为(batch_size * 1 * tgt_seq_len * tgt_seq_len)\n",
    ">     + attention中score的维度为 batch_size * head_num * tgt_seq_len* tgt_seq_len\n",
    "> + Decoder里的Encoder-Decoder Attention：计算src_seq 和 tgt_seq之间的交叉注意力。输入 query维度 (batch_size * tgt_seq_len * d_model), key和value的维度为(batch_size * tgt_seq_len * d_model)。mask维度为 (batch_size * 1 * src_seq_len)。由于前一层 Masked-Self-Attention 已经提供了tgt_mask,屏蔽了当前时刻之后的序列，因此这里输入src_mask\n",
    ">     + attention的输入为 query的维度为(batch_size * head_num * tgt_seq_len * head_dim)，key, value 的维度为(batch_size * head_num * src_seq_len * head_dim)，mask维度为(batch_size * 1 * 1 * src_seq_len)\n",
    ">     + attention中score的维度为 batch_size * head_num * tgt_seq_len * src_seq_len\n",
    "\n",
    "> **Question 3:** Mask共有几类，它们的作用是什么<br/>\n",
    "> **Answer:** Mask共有两类\n",
    "> + pad_mask:屏蔽掉填充位\n",
    "> + attn_mask：遮盖当前时刻之后的序列。注意：Decoder中Encoder-Decoder Attention输入是pad_mask(src_mask)，但前一个layer中有tgt_mask(attn_mask)起作用\n",
    "\n",
    "> **Question 4:** Embedding为什么要乘以$\\sqrt{d_{model}}$ <br/>\n",
    "> **Answer:** 作者原本想法是增大Embedding的差异性，不至于在训练过程中退化。但有人试验过这个似乎并没有用。\n",
    "\n",
    "> **Question 5:** 为什么要有Positional Encoding？<br/>\n",
    "> **Answer:** 因为Attention是位置无关的，为了引入元素位置信息，需要Positional Encoding。其中Positional Encoding是写死的，pe维度为(batch_size * max_len * d_model), 直接与Embedding的结果相加 \n",
    "\n",
    "> **Question 6:** LayerNorm中W为何是向量？ <br/>\n",
    "> **Answer:** 因为这里没有发生维度变化，因此可以采用向量点乘\n",
    "\n",
    "> **Question 7:** Label Smoothing的作用？<br/>\n",
    "> **Answer:** 缓解One-Hot编码存在的问题。具体详见OnlineNotes-LabelSmoothing\n",
    "\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"graphic_files/rnn_framework.jpg\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图1 Transformer工作流程（RNN架构）</div>\n",
    "</center>\n",
    "\n",
    "## 一、Transformer工作流程\n",
    "\n",
    "Transformer工作有流程如上图所示，主要有以下特点\n",
    "+ 整体来讲分为Encoder和Decoder两大部分，每个部分有多个Layer层叠，可以分别从下至上编码为0~5。层叠数目6是实验出来的，没有特殊之处\n",
    "+ 每一个Layer不共享参数\n",
    "+ 在工作过程中，EncoderEncoder输出的结果称之为memory，之后传递给每一个Decoder，前一层Decoder的解码结果传递给后一层\n",
    "+ Transformer所谓的并行是特殊的并行，具体讲解在[知乎：浅析Transformer训练时并行问题](https://zhuanlan.zhihu.com/p/368592551)\n",
    "    - RNN需要串行是因为其t时刻依赖t-1时刻的输出。\n",
    "    - Transformer中的Encoder支持并行化。因为Encoder核心是自注意力机制，即计算任意两个元素$x_i$和$x_j$之间的权重,$i$和$j$可以相等。这样一来序列$z_{seq}$就可以转化为一系列权重的加和\n",
    "    - Decoder只有在训练过程中支持并行化。因为其采用了**teacher force**和**masked-self-attention**机制。前者强制每一个Decoder在解码过程中使用正确的信息。后者则负责将解码过程中的正确信息逐步展开。在预测阶段，则退化为前后依赖的串行模式。\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"graphic_files/Transformer_framework_all.jpg\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图2 Transformer结构简图</div>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"graphic_files/Transformer_all_1.jpg\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图3 Transformer结构划分图</div>\n",
    "</center>\n",
    "\n",
    "## Transformer具体架构\n",
    "如图2所示，整体来讲，Transformer分为6个主要部分，12个基本组件。\n",
    "\n",
    "### Transformer主要部分\n",
    "+ EncoderDecoder:这个是总类，协调以下部分\n",
    "+ Encoder部分：负责将输入部分转化为memory\n",
    "+ Decoder：根据memory逐步解码出logits\n",
    "+ Generator：根据logits通过softmax输出top-N序列\n",
    "+ Embedding和PositionalEncoding，将序列转化为embedding\n",
    "    - 对于Encoder，是输入序列的word embedding + positional encoding\n",
    "    - 对于Decoder，在训练阶段是完整的输出序列+mask；在实际测试阶段，是当前时刻的解码序列+mask\n",
    "\n",
    "### Transformer具体组件\n",
    "1. Embedding: 是将输入的tokens转化为编码\n",
    "2. PositionalEncoding: 是为编码加入位置信息。由于Attention是位置无关的，因为在编码中加入PositionalEncoding，且PositionalEncoding是写死的，无需训练。直接将结果与word embedding相加\n",
    "3. MultiHeadAttention: 计算任意两个位置对应元素之间的关系。个人认为总共包括三类：Encoder的Self-Attention，Decoder的Self-Attention，Decoder的Encoder-Decoder Attention\n",
    "4. SubLayerConnection：链接Sublayer，包括残差(Add)和归一(Norm)\n",
    "5. PositionwiseFeedForward：全连接网络\n",
    "6. EncoderLayer：单层Encoder，主要包括两个Sublayer，Self-Attention及FFN\n",
    "7. LayerNorm：归一Norm\n",
    "8. Encoder：编码器，多个EncoderLayer堆叠\n",
    "9. DecoderLayer：单层解码器，包括三个Sublayer：Masked Self-Attention，Encoder-Decoder Attention，FFN\n",
    "10. Decoder：解码器，多个DecoderLayer堆叠\n",
    "11. Generator：Decoder的输出为logits，即某个位置概率得分最大，其他位置可以非0，经过Generator后输出概率。Decoder输出的维度为 batch_size * seq_len * d_model，经generator后输出维度为 batch_size * seq_len * tgt_vocab_size。具体取最后一个位置，由greedy_decode控制\n",
    "\n",
    "下面是对Transformer代码的具体介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd86f4-a005-4c83-9f66-941796546177",
   "metadata": {},
   "source": [
    "## 二、 Transformer Implementation\n",
    "\n",
    "### 0. import部分\n",
    "其中seaborn和matplotlib只是用于画图展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ecb61-3f43-4acf-8bbd-6ef74b067a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "seaborn.set_context(context='talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f096f-7f03-4249-8802-2a177498e9a9",
   "metadata": {},
   "source": [
    "### 1. Embedding部分\n",
    "\n",
    "Embedding主要负责将序列进行编码\n",
    "\n",
    "在forward中，embedding乘以$\\sqrt{d_{model}}$目的是为了使Embedding的差异化更明显，不至于在训练过程中退化。但作者经过验证，该步骤似乎没有效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529cedcb-6b1c-4dfe-bb61-ae35fcd04c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        '''\n",
    "        :param d_model: scalar，表示模型的维度，这里默认为512\n",
    "        :param vocab: scalar，表示语言的词汇表的大小\n",
    "        '''\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        '''\n",
    "        one-hot转词嵌入，这里有一个待训练的矩阵E，大小是vocab*d_model。\n",
    "        这样可以使单条seq的表征从 seq_len * vocab -> seq_len * d_model\n",
    "        '''\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x：表示输入序列batch，维度为 batch_size * seq_len* vocab_size\n",
    "        '''\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a68bc1-7679-49c4-b2ff-766b2e9f69d0",
   "metadata": {},
   "source": [
    "### 2. PositionalEncoding部分\n",
    "\n",
    "PositionalEncoding是写死的，计算公式如下\n",
    "$$\n",
    "\\begin{cases}\n",
    "    PE(pos,2i) = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\\\\\n",
    "    PE(pos,2i+1) = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "上式括号内参数可以简化为：\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{pos}{\\alpha} &= pos \\times \\frac{1}{10000^{\\frac{2i}{d_{model}}}} = pos \\times e^{\\log{10000^{\\frac{2i}{d_{model}}}}} \\\\\n",
    "&= pos \\times e^{-\\log{10000^{\\frac{2i}{d_{model}}}}} = pos \\times e^{-\\frac{2i}{d_{model}}\\log{10000}} = pos \\times e^{2i \\times (- \\frac{\\log{10000}}{d_{model}})}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48e1eb-32d3-435f-92af-dfe64a3e93c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        '''\n",
    "        d_model: 表示模型维度，默认512\n",
    "        dropout: dropout概率，标量\n",
    "        max_len：序列的最大长度，标量\n",
    "        '''\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        '''\n",
    "        pe 的维度之所以要是 max_len * d_model，是因为它对所有的Sequence都是一样的。\n",
    "        因此对于任意序列 seq={x_1, x_2, ..., x_m}，每一个元素 x_i 都是 d_model 维，这样就可以直接加到序列的embedding上了\n",
    "        '''\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        '''\n",
    "        这一行是得到 max_len * 1 的数组， 元素从0到4999\n",
    "        '''\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)    # 这样使得维数从 max_len * d_model 变为 1 * max_len * d_model, 为batch_size留出位置\n",
    "        '''\n",
    "        unsqueeze(0)使得维度从 max_len * d_model 变为 1 * max_len * d_model，目的是为batch_size留出位置\n",
    "        '''\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        '''\n",
    "        这一步是将 x 和 pos_emb 之后的矩阵相加\n",
    "        x的维度为 batch_size * seq_len * d_model\n",
    "        Variable(*)的维度为 1 * max_len * d_model\n",
    "        \n",
    "        pe[:,:x.size(1)]表示第一维全取，第二维取 max_len 和实际 size的最小值，第三维没写即全取\n",
    "        这里要注意的是pe是固定的，固定的值加到每一个batch的每一个Sequence上\n",
    "        '''\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d8de5-ad0a-49cf-a451-e270be85ed99",
   "metadata": {},
   "source": [
    "### 3. MultiHeadedAttention部分\n",
    "\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"graphic_files/attention.jpg\" height=\"300\" width=\"300\">\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"graphic_files/multi_attention.jpg\" height=\"300\" width=\"300\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图4 Attention与MultiHeadedAttention计算图</div>\n",
    "</center>\n",
    "\n",
    "\n",
    "首先先看Attention的计算方式，如上图所示，其计算公式如下：\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_{model}}})V$$\n",
    "MultiHeadedAttention有四个要训练的矩阵，分别如上图所示。\n",
    "\n",
    "#### <font color=red>【个人理解】Attention的一些要点</font>\n",
    "+ <font color=red>注意力的目的是计算任意两个元素之间的关系</font>\n",
    "+ <font color=red>attention 和 MultiHeadedAttention均可以用于计算自注意力(Q,K,V均来自于同一个序列src_seq或tgt_seq, 图1 左侧Stage2和右侧的Stage2)和交叉注意力(Q来自trg_seq;K,V来自src_seq，右侧的Stage3)</font>\n",
    "+ <font color=red>对于交叉注意力（由src_seq生成trg_seq），src_seq提供key(K)，供trg_seq来query(Q)，这样得到一个分数（score），最后再与来自src_seq的value(V)相乘，得到trg_seq的表达。这里之所以要最后乘以V，是为了QK计算完Score（权重）后与V相乘，成倍数地放缩V，使V值相对完整（相对固定）</font>\n",
    "+ <font color=red>mask分为pad_mask和attn_mask，具体关于mask的一些讲解，可以参考[知乎：transformer中: self-attention部分是否需要进行mask](https://www.zhihu.com/question/472323371/answer/2001223766)</font>\n",
    "    - <font color=red>在Encoder Self-Attention阶段（图1左侧Stage2），mask为pad_mask，即pad填充的mask, 传入的维度为 batch_size * 1 * src_seq_len；在attention函数中，mask维度为 batch_size * 1 * 1 * src_seq_len, scores的维度为 batch_size * head_num * src_seq_len * src_seq_len</font>\n",
    "    - <font color=red>在Decoder Masked Self-Attention阶段(图1右侧Stage2)，mask为tgt_seq的下三角矩阵，维度为 batch_size * trg_seq_len * trg_seq_len；在attention函数中，mask维度为batch_size * 1 * trg_seq_len * trg_seq_len，scores的维度为 batch_size * head_num * tgt_seq_len * tgt_seq_len</font>\n",
    "    - <font color=red>在Decoder Encoder-Decoder Attention阶段(图1右侧Stage3)，由于Masked Self-Attention已经包含了tgt_seq及其mask，因此这里传入src_mask计算src_seq和tgt_seq之间的关系。mask为src的pad_mask，维度为batch_size * 1 * src_seq_len; attention函数中mask维度为 batch_size * 1 * 1 * src_seq_len；scores的维度为 batch_size * head_num * tgt_seq_len * seq_seq_len</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b60ef2-942f-42cf-af90-f66f699b8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    '''\n",
    "    query：来自trg_seq, 维度为 batch_size * head_num * tgt_vocab * d_model\n",
    "    key和value：来自src_seq，维度为 batch_size * head_num * src_vocab * d_model\n",
    "    \n",
    "    '''\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    '''\n",
    "    scores的对于交叉注意力来说，维度为 batch_size * head_num * tgt_vocab * src_vocab\n",
    "    '''\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        '''\n",
    "        这里扩展是为了给multi-head留下空间\n",
    "        '''\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in\n",
    "                             zip(self.linears, (query, key, value))]\n",
    "\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183a21d-ebbc-4456-b7db-a01454727611",
   "metadata": {},
   "source": [
    "### 4. SublayerConnection部分\n",
    "主要作用是是先残差Add以及Norm（正则化）。其中Norm通过 7.LayerNorm实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648b8ce-8d60-4c85-bcd8-c91ada51a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        '''\n",
    "        size：d_model\n",
    "        '''\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        '''\n",
    "        x: 处理的数据，维度为 batch_size * seq_len * d_model\n",
    "        sublayer: 可以是MultiHeadedAttention或者PositionwiseFeedForward\n",
    "        '''\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab9efe-d374-4aeb-add7-fcb8e3bdc4d7",
   "metadata": {},
   "source": [
    "### 5. PositionwiseFeedForward部分\n",
    "这里的处理公式为\n",
    "$$\n",
    "\\text{FFN}(x) = max(0, x W_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "其中max相当于ReLU激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca6f8c-0f2a-489a-9b53-b9f721a94548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        '''\n",
    "        d_model =512\n",
    "        d_ff = 2048\n",
    "        '''\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d30079-9dc2-4dc1-893e-f3be9c14bf3c",
   "metadata": {},
   "source": [
    "### 6. EncoderLayer部分\n",
    "\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"graphic_files/encoder.png\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图5 EncoderLayer结构简图</div>\n",
    "</center>\n",
    "\n",
    "这里注意下MultiHeadedAttention的输入，输入是x，x，x，mask。\n",
    "这里x的维度为batch_size * seq_len * d_model\n",
    "mask的维度为 batch_size * 1 * seq_len。 作用是屏蔽掉pad填充的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8488b-f45b-4187-b158-2092887284c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        '''\n",
    "        x维度：batch_size * seq_len* d_model\n",
    "        mask维度： \n",
    "        '''\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c305ba31-6cb8-418e-aaa7-58692c5a4af2",
   "metadata": {},
   "source": [
    "### 7. LayerNorm部分\n",
    "这里要注意, 和nn.Linear及nn.Embedding不同，这里的Norm参数是weight_vector（向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadf76c-cbe5-4fbd-b9a7-d63415150838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        '''\n",
    "        features: d_model\n",
    "        eps: 用于分母的非0化平滑\n",
    "        '''\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        '''\n",
    "        a_2 是可训练的向量\n",
    "        '''\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        '''\n",
    "        b_2 是可训练的参数向量\n",
    "        '''\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "        '''\n",
    "        (x-mean)/std\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0cc02-d48b-4341-b2ca-cec6f0663457",
   "metadata": {},
   "source": [
    "### 8. Encoder部分\n",
    "N个EncoderLayer叠加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8c6ae-fd5f-44eb-a5a4-c59d407e84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06006f47-fc12-40b2-8799-636a91e5cbce",
   "metadata": {},
   "source": [
    "### 9. DecoderLayer部分\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"graphic_files/decoder.png\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图6 DecoderLayer结构简图</div>\n",
    "</center>\n",
    "这里可以看到，DecoderLayer主要分为三个Sublayer，从下往上分别为masked self-attention，encoder-decoder attention，feed forward network\n",
    "+ Masked Self-Attention: 输入为 tgt_seq, tgt_seq, tgt_seq, tgt_mask。\n",
    "    - tgt_seq: batch_size * tgt_seq_len * d_model\n",
    "    - tgt_mask: batch_size * tgt_seq_len * tgt_seq_len\n",
    "+ Encoder-Decoder Attention: 输入为 tgt_seq, memory, memory, src_mask。\n",
    "    - tgt_seq: batch_size * tgt_seq_len * d_model\n",
    "    - memory: batch_size * src_seq_len * d_model\n",
    "    - tgt_mask: batch_size * 1 * src_seq_len\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c29d0-f0e7-44c6-b741-9bfab3610c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ac06f-4149-4639-bf4a-3ef62d1d5ae5",
   "metadata": {},
   "source": [
    "### 10. Decoder部分\n",
    "N个DecoderLayer叠加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870e3d3-2f82-4652-81a4-12715a6cc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f9c71-e3c3-4ec2-8206-148e344fb66f",
   "metadata": {},
   "source": [
    "### 11. Generator部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5f0c7-3422-4f08-b817-e9fda16295f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        '''\n",
    "\n",
    "        :param d_model: scalar\n",
    "        :param vocab: scalar\n",
    "        '''\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: tensor, batch * seq_len * d_model\n",
    "        :return: batch_size * seq_len * vocab\n",
    "        '''\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d878634-037d-48f1-be2d-eb0eba4005e3",
   "metadata": {},
   "source": [
    "### 12. EncoderDecoder部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0cbf3-243e-4aad-a913-18b6111c49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfa95a-b3d0-4058-bc69-af8d56f98de5",
   "metadata": {},
   "source": [
    "### 13. make_model部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f20a0f-4685-4e41-ab30-c1b15dd3c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2efb6-9f8b-452d-9cc8-5cbd5f41da2b",
   "metadata": {},
   "source": [
    "## 三、 部分功能函数\n",
    "\n",
    "### 1. clones 深度拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff8cf0-1048-44d8-8476-a59b4fefa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b9ac2-bde1-42ff-bb45-a9a82c102d07",
   "metadata": {},
   "source": [
    "### 2. subsequent_mask tgt下三角掩码\n",
    "制作针对tgt的下三角掩码矩阵。目的是为了遮盖下一时刻之后的输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130672a-d40b-4cb5-8eb0-8c408c0f5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda2798-02ea-4ac3-aff0-db0ebdb1aca7",
   "metadata": {},
   "source": [
    "### 3. Label Smoothing\n",
    "编码方式如下，原因间Label Smoothing笔记\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_{true} = 1-\\epsilon\\\\\n",
    "y_{false} = \\frac{\\epsilon}{K-1}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56dbea9-4308-41ac-852e-ab86d94ca049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efbaf5-8730-4e3b-9a8d-4694c9504e25",
   "metadata": {},
   "source": [
    "### 3. NoamOpt优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41935498-9770-41bb-b92a-0721cea2172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8376c-7dcc-49f3-b871-148f68003dc7",
   "metadata": {},
   "source": [
    "### 4. SimpleLossCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fa255-dedb-4390-aa6b-d899f505cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion, opt = None):\n",
    "        '''\n",
    "        criterion = LabelSmoothing(size=tgt_V, padding_idx=0, smoothing=0.0)\n",
    "        '''\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1))/norm\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data.item()*norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0d845-cd55-4af8-8105-2fe292fc00cb",
   "metadata": {},
   "source": [
    "### 5. greedy_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29037e19-b9d0-4ddf-b6a6-7e14d0e1489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1,1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(memory, src_mask, Variable(ys), Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        '''\n",
    "        out 这里是logits\n",
    "        '''\n",
    "        prob = model.generator(out[:, -1])\n",
    "        '''\n",
    "        prob这里转化为index。\n",
    "        其中out[:,-1]表示batch全取，seq_len中只取最后一个位置\n",
    "        '''\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        '''\n",
    "        找到最大的, 第一个返回值为最大值，第二个返回值为对应的索引\n",
    "        '''\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(next_word)], dim = 1)\n",
    "        '''\n",
    "        拼接到上一轮次\n",
    "        '''\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58d36b-787a-4a2a-aa84-e0013ffe9c8b",
   "metadata": {},
   "source": [
    "## 运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82aca6-c722-4d35-8bf8-3ad8d0f41862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
